{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Desafio Engenheiro de Dados.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBxr3A52hNVydZEg84XBTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laroccathebrux/DesafioEngenheiroDados/blob/main/Desafio_Engenheiro_de_Dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LzfM7YWceh"
      },
      "source": [
        "# Desafio Engenheiro de Dados #\n",
        "\n",
        "O desafio consiste em criar um snippet de código e responder uma pergunta sobre como escalar essa solução. A disponibilização do snippet e resposta da pergunta podem ser feitas via algum site que permita o compartilhamento de código, GitHub ou até mesmo pelo e-mail.\n",
        "\n",
        "### Estratégia ###\n",
        "Para realizar este desafio, separei as ações em 4 etapas:\n",
        "- Salvar dados em arquivos Json\n",
        "- Usando PySpark ler os dados dos arquivos Json e criar views que permitam o uso de linguagem SQL\n",
        "- Fazer o cálculo em um único SQL criando um JOIN entre as duas views criadas\n",
        "- Responder a pergunta final do desafio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GLuWoorXk_8"
      },
      "source": [
        "# Etapa 1 #\n",
        "---\n",
        "Os arquivos foram salvos em formato Json com os seguintes nomes:\n",
        "- contratos.json\n",
        "- transacoes.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVuK7vNEXxH8"
      },
      "source": [
        "# Etapa 2 #\n",
        "---\n",
        "Para que possamos utilizar o PySpark, primeiro precisamos instalar a biblioteca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfCPKNZ99bpJ",
        "outputId": "9d015697-cdae-4abe-db7c-550401995607"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 72kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=60937dfa024c5d0096945a1d51726b6962ef15413a1caf6d2decb785557557eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0fxZEx0X-qk"
      },
      "source": [
        "### Importação das bibliotecas ###\n",
        "Agora que temos o PySpark instalado, precisamos importar as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0j0ETHT9S-L"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import desc\n",
        "from pyspark.sql.functions import asc\n",
        "from pyspark.sql.functions import sum as Fsum\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8wIDmXPYJqI"
      },
      "source": [
        "### Criação do Objeto spark ###\n",
        "Foi criado um objeto chamado spark que será usado por todo o código para realizar as ações do Apache Spark.\n",
        "O nome do aplicativo escolhido foi 'Desafio Engenheiro de Dados'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmJjTaxy9oOg"
      },
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Desafio Engenheiro de Dados\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOHWjnatYbXl"
      },
      "source": [
        "### Importação dos arquivos Json ###\n",
        "Ambos arquivos são importados para dois dataframes:\n",
        "- df_t = lê os dados do arquivo transacoes.json\n",
        "- dt_c = lê os dados do arquivo contratos.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8n59M-n9xOI"
      },
      "source": [
        "path = \"./transacoes.json\"\n",
        "df_t = spark.read.json(path)\n",
        "path = \"./contratos.json\"\n",
        "df_c = spark.read.json(path)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQxwa3HMYtqT"
      },
      "source": [
        "### Validar Data Types e Null Values ###\n",
        "Imprimindo o Schema de df_t e df_c para verificar os data types encontrados automaticamente pelo Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHqLtSuWADTv",
        "outputId": "dabb1e14-317d-4a02-f283-e145e15c0c82"
      },
      "source": [
        "df_t.printSchema()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- client_id: long (nullable = true)\n",
            " |-- discount_percentage: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- transaction_id: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnGeNLHFBcpI",
        "outputId": "173044da-6eb8-4080-b191-c6d2a93d2a1a"
      },
      "source": [
        "df_c.printSchema()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- client_id: long (nullable = true)\n",
            " |-- client_name: string (nullable = true)\n",
            " |-- contract_id: long (nullable = true)\n",
            " |-- is_active: boolean (nullable = true)\n",
            " |-- percentage: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTfR49emZFrr"
      },
      "source": [
        "Uma vez que temos apenas um valor null em nosso dataset e o mesmo pode ser tratado como um valor double 0, vamos substituí-lo de acordo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGP35zyzTJhs"
      },
      "source": [
        "df_t = df_t.fillna({'discount_percentage':'0'})"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvDIbCVMZSCz"
      },
      "source": [
        "Vamos mudar o campo total_amount de long (int) para double uma vez que, apesar de possuir valor inteiros, ele é um campo de valor financeiro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emvK7TBZUTT7"
      },
      "source": [
        "df_t = df_t.withColumn(\"total_amount\", df_t[\"total_amount\"].cast(DoubleType()))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FaePTGOZbh7"
      },
      "source": [
        "### Criar Views ###\n",
        "Vamos criar duas views baseadas em ambos data frames criados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpbpyP6jCEUu"
      },
      "source": [
        "df_t.createOrReplaceTempView(\"transactions\")\n",
        "df_c.createOrReplaceTempView(\"contracts\")"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVgS484rZlSw"
      },
      "source": [
        "Verificando a criação da **view Transactions**\n",
        "\n",
        "**ATENÇÃO**: Para datasets com mais dados, utilizar na query um LIMIT com valor, para evitar problemas de performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgYdo7blCEWR",
        "outputId": "4e2f826c-0aa7-4645-81e2-00846f3f352b"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT * \n",
        "          FROM transactions \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-------------------+------------+--------------+\n",
            "|client_id|discount_percentage|total_amount|transaction_id|\n",
            "+---------+-------------------+------------+--------------+\n",
            "|     3545|               6.99|      3000.0|             1|\n",
            "|     3545|               0.45|      4500.0|             2|\n",
            "|     3509|                0.0|     69998.0|             3|\n",
            "|     3510|                0.0|         1.0|             4|\n",
            "|     4510|               40.0|        34.0|             5|\n",
            "+---------+-------------------+------------+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hjN5kRHZ9fB"
      },
      "source": [
        "Verificando a criação da **view Contracts**\n",
        "\n",
        "**ATENÇÃO**: Para datasets com mais dados, utilizar na query um LIMIT com valor, para evitar problemas de performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8hVZPYwDbxs",
        "outputId": "55e37558-a164-498e-edad-221265b9e7c5"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT * \n",
        "          FROM contracts \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------------+-----------+---------+----------+\n",
            "|client_id|    client_name|contract_id|is_active|percentage|\n",
            "+---------+---------------+-----------+---------+----------+\n",
            "|     3545| Magazine Luana|          3|     true|       2.0|\n",
            "|     3545| Magazine Luana|          4|    false|      1.95|\n",
            "|     3509|Lojas Italianas|          5|     true|       1.0|\n",
            "|     3510|      Carrefive|          6|     true|       3.0|\n",
            "+---------+---------------+-----------+---------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFCbQ3VAaA_D"
      },
      "source": [
        "# Etapa 3 - Cálculo #\n",
        "A estratégia para o cálculo foi somar os valores das views respeitando os seguintes critérios:\n",
        "- valor de total_amount subtraído da porcentagem de desconto\n",
        "- resultado anterior multiplicado pela porcentagem do contrato\n",
        "- respeitando apenas os registros cujo campo is_active for igual a True\n",
        "- Os registros de transactions que não existem em contracts, não entrarão no cálculo devido ao JOIN realizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYY40pDnUzHi",
        "outputId": "992d4f73-f5f3-4ffa-abbe-676c3be0002c"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT SUM(((t.total_amount-(t.total_amount*discount_percentage/100))*c.percentage/100)) AS total_amount FROM contracts AS c\n",
        "          INNER JOIN transactions as t ON t.client_id = c.client_id\n",
        "          WHERE c.is_active=true\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|     total_amount|\n",
            "+-----------------+\n",
            "|845.4110000000001|\n",
            "+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pz0i5meawSC"
      },
      "source": [
        "# Etapa 4 - Resposta da Pergunta #\n",
        "\n",
        "Além do código acima, considere que uma escala de ~200 milhões de transações por dia e que o cálculo deverá apresentar um resultado do valor total do mês. Descreva em até 500 palavras que tecnologias e arquitetura você usaria para escalar a solução acima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjxdxFALa3pb"
      },
      "source": [
        "Num caso de ~200 milhões de transações dia e sabendo que os dados capturados serão utilizados para outros fins além do cálculo apresentado, seria prudente utilizar alguma solução de streaming de dados.\n",
        "Porém a pergunta deste desafio não especifica outras regras de negócio, portanto me basearei na ideia de que iremos apenas calcular o total_amount dos dados recebidos.\n",
        "\n",
        "**Premissa**\n",
        "\n",
        "São ~200 milhões de transações dia, portanto precisaremos fazer mais de um cálculo para evitar computar ~6 bilhões de dados mês. Sendo assim, devemos fazer o seguinte:\n",
        "- Gravar um arquivo json com os registros em um storage (Azure Blob ou AWS S3), formando assim nosso data lake.\n",
        "- Com o uso do Apache Spark e Python, vamos criar um cluster para poder processar os dados de modo distribuído (Azure Databricks ou AWS Redshift)\n",
        "- Após o cálculo feito, salvar o registro em um Banco de dados. Neste caso não há necessidade de um banco de dados não relacional, podendo ser utilizado um Azure SQL Server ou AWS Postgres. Mas caso seja necessário o uso de um banco no-sql, então podemos utilizar o Azure CosmosDB ou AWS com Apache Cassandra.\n",
        "- Criaremos um novo arquivo python que lerá os dados do banco de dados escolhido e somará os resultados obtidos\n",
        "- Para que tudo isso faça sentido, criaremos 2 data pipelines no Apache Airflow, o primeiro rodando diariamente, fazendo o calculo e salvando no banco de dados. O segundo rodando mensalmente, chamando o segundo arquivo python que faz a soma dos registros guardados no banco.\n",
        "- Para realizar os cálculos nos meses seguintes, podemos adicionar uma data nos registros guardados, ou podemos simplesmente fazer um drop das tabelas todos os meses.\n",
        "\n",
        "**Ponderações**\n",
        "- Sabemos que a utilização de JOIN entre tabelas é um recurso bastante útil, mas estremamente lento, portanto o código acima deverá ser adaptado para poder executar o cálculo de maneira mais eficiente. (um loop em ambos Data Frame seria suficiente para realizar o calculo de maneira satisfatória)\n"
      ]
    }
  ]
}